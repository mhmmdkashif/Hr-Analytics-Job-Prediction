{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2334549,"sourceType":"datasetVersion","datasetId":1409252}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mhmmdkashif/notebook6c2782092b?scriptVersionId=195064768\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Import packages\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For displaying all of the columns in dataframes\npd.set_option('display.max_columns', None)\n\n# For data modeling\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# For metrics and helpful functions\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV, learning_curve\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,\\\nf1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report,\\\nprecision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom sklearn.tree import plot_tree\nfrom sklearn.model_selection import cross_val_score\nfrom imblearn.over_sampling import SMOTE\n\n# For saving models\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-02T18:54:17.941367Z","iopub.execute_input":"2024-09-02T18:54:17.941794Z","iopub.status.idle":"2024-09-02T18:54:17.952587Z","shell.execute_reply.started":"2024-09-02T18:54:17.941751Z","shell.execute_reply":"2024-09-02T18:54:17.951052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RUN THIS CELL TO IMPORT YOUR DATA. \n\n# Load dataset into a dataframe\n### YOUR CODE HERE ###\ndf0 = pd.read_csv(\"/kaggle/input/hr-analytics-and-job-prediction/HR_comma_sep.csv\")\n\n\n# Display first few rows of the dataframe\ndf0.head ()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:54:17.954621Z","iopub.execute_input":"2024-09-02T18:54:17.955096Z","iopub.status.idle":"2024-09-02T18:54:18.041922Z","shell.execute_reply.started":"2024-09-02T18:54:17.955038Z","shell.execute_reply":"2024-09-02T18:54:18.040692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display all column names\ndf0.info ()\n\n# Rename columns as needed\ndf0.columns = [col.strip().replace(' ', '_').replace('-', '_').lower() for col in df0.columns]\n\n# Drop duplicates and save resulting dataframe in a new variable as needed\ndf0_cleaned = df0.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:54:18.04389Z","iopub.execute_input":"2024-09-02T18:54:18.044286Z","iopub.status.idle":"2024-09-02T18:54:18.087191Z","shell.execute_reply.started":"2024-09-02T18:54:18.044247Z","shell.execute_reply":"2024-09-02T18:54:18.086059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determine the number of rows containing outliers\n# Calculate Q1 (25th percentile) and Q3 (75th percentile)\nQ1 = df0_cleaned['time_spend_company'].quantile(0.25)\nQ3 = df0_cleaned['time_spend_company'].quantile(0.75)\n\n# Calculate the Interquartile Range (IQR)\nIQR = Q3 - Q1\n\n# Determine the lower and upper bounds for outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Identify rows with outliers in the `time_spend_company` column\noutliers = df0_cleaned[(df0_cleaned['time_spend_company'] < lower_bound) | (df0_cleaned['time_spend_company'] > upper_bound)]\n\n# Count the number of rows containing outliers\nnum_outliers = outliers.shape[0]\nprint(\"Number of rows containing outliers in `time_spend_company`:\", num_outliers)\n\n# Filter the DataFrame to remove outliers\ndf_filtered = df0_cleaned[(df0_cleaned['time_spend_company'] >= lower_bound) & (df0_cleaned['time_spend_company'] <= upper_bound)].reset_index (drop=True)\n\ndf_filtered['salary'] = df_filtered['salary'].astype('category').cat.set_categories(['low', 'medium', 'high']).cat.codes\ndf_filtered = pd.get_dummies(df_filtered, drop_first=False)\ndf_filtered","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:54:18.088459Z","iopub.execute_input":"2024-09-02T18:54:18.088805Z","iopub.status.idle":"2024-09-02T18:54:18.141351Z","shell.execute_reply.started":"2024-09-02T18:54:18.088768Z","shell.execute_reply":"2024-09-02T18:54:18.140195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a plot as needed\n#HR domain knowledge is used to determine that the important columns for predicting turnover in this case are 'satisfaction_level', 'number_project', 'average_montly_hours', and 'time_spend_company'.\n# Calculate the correlation matrix\ncorr_matrix = df_filtered[['satisfaction_level', 'number_project', 'average_montly_hours', 'time_spend_company', 'left']].corr()\n\n# Extract correlations with the 'left' column\nleft_corr = corr_matrix['left'].sort_values(ascending=False)\n\n# Display the correlation values with 'left'\nprint(\"Correlation with 'left':\\n\", left_corr)\n\n# Visualize the correlation matrix focusing on 'left'\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix[['left']], annot=True, cmap='coolwarm', cbar=True)\n\n# Add a title\nplt.title('Correlation of Variables with Employee Turnover (left)')\n\n# Display the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T22:15:58.502792Z","iopub.execute_input":"2024-09-02T22:15:58.503776Z","iopub.status.idle":"2024-09-02T22:15:58.963343Z","shell.execute_reply.started":"2024-09-02T22:15:58.503717Z","shell.execute_reply":"2024-09-02T22:15:58.961263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a plot as needed\n\n# Select the numerical variables and 'left' for the pairplot\nselected_columns = ['satisfaction_level', 'number_project', \n                    'average_montly_hours', 'time_spend_company', 'left']\n\n# Create the pairplot\nsns.pairplot(df_filtered[selected_columns], hue='left', palette='Set1', diag_kind='kde')\n\n# Add a title to the plot\nplt.suptitle('Pairplot of Selected Variables Colored by Turnover (left)', y=1.02)\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-02T20:18:28.196036Z","iopub.execute_input":"2024-09-02T20:18:28.197041Z","iopub.status.idle":"2024-09-02T20:18:48.254688Z","shell.execute_reply.started":"2024-09-02T20:18:28.196986Z","shell.execute_reply":"2024-09-02T20:18:48.253289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The pairplots show a strong correlation between independent variables average_monthly_hours and number_project. \n# Features and target\nX = df_filtered[['satisfaction_level', 'number_project', 'average_montly_hours', 'time_spend_company']]\n#X = df_filtered.drop ('left', axis=1)\ny = df_filtered['left']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n\n# Apply SMOTE to the training data to address imbalance\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n\n# Model training\nmodel = LogisticRegression(random_state=42, max_iter=500, class_weight='balanced').fit(X_resampled, y_resampled)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Model evaluation\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy}')\nprint('Confusion Matrix:')\nConfusionMatrixDisplay(confusion_matrix=conf_matrix, \n                                  display_labels=model.classes_).plot (values_format='')\nplt.show ()\nprint (classification_report(y_test, y_pred, target_names=['Predicted would not leave', 'Predicted would leave']))\n\ncv_scores = cross_val_score(model, X, y, cv=5, scoring='recall')\nprint(f'Cross-Validation Recall: {cv_scores.mean()} ± {cv_scores.std()}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T22:20:58.389584Z","iopub.execute_input":"2024-09-02T22:20:58.390155Z","iopub.status.idle":"2024-09-02T22:20:59.60829Z","shell.execute_reply.started":"2024-09-02T22:20:58.390101Z","shell.execute_reply":"2024-09-02T22:20:59.606342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the Random Forest model and get its internal validation estimate\nrf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\nrf.fit(X_train, y_train)\nprint(f'OOB Score: {rf.oob_score_:.4f}')\n\n# Narrower Hyperparameter grid\nparam_grid = {\n    'max_depth': [3, 5, 10],\n    'max_features': ['sqrt', 'log2'],  # Fraction of features, usually sqrt(log2) is faster\n    'max_samples': [0.7, 0.9],  # Reduced the samples to train each tree on\n    'min_samples_leaf': [2, 3],  # Pruning the tree early\n    'min_samples_split': [3, 4],\n    'n_estimators': [100, 200],  # Fewer trees to reduce complexity\n}\n\n# Use RandomizedSearchCV for faster hyperparameter tuning\nrandom_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=50, \n                                   cv=3, scoring='roc_auc', refit=True, random_state=42, n_jobs=-1)\nrandom_search.fit(X_train, y_train)\n\n# Best model from RandomizedSearchCV\nbest_rf = random_search.best_estimator_\n\n# Predictions\ny_pred_rf = best_rf.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred_rf)\nprint(f'Random Forest Model Accuracy: {accuracy:.4f}')\nprint(classification_report(y_test, y_pred_rf, target_names=['Predicted would not leave', 'Predicted would leave']))\ncv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='roc_auc')\nprint(f'Cross-Validation AUC Scores: {cv_scores}')\nprint(f'Mean AUC Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}')\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred_rf)\nConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=best_rf.classes_).plot(values_format='')\nplt.show()\n\n# Print the best parameters\nprint(f'Best Parameters: {random_search.best_params_}')\n\n# Predictions: Get the predicted probabilities for the positive class\ny_pred_proba = best_rf.predict_proba(X_test)[:, 1]\n\n# Calculate the ROC AUC score\nroc_auc = roc_auc_score(y_test, y_pred_proba)\nprint(f'Random Forest Model ROC AUC: {roc_auc:.4f}')\n\n# Optional: Plot the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for reference\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\navg_precision = average_precision_score(y_test, y_pred_proba)\nplt.plot(recall, precision, label=f'Precision-Recall curve (AP = {avg_precision:.4f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc='lower left')\nplt.show()\n\nimportances = best_rf.feature_importances_\nfeature_names = X_train.columns\n\n# Sort features and importances together by importance (descending order)\nsorted_idx = importances.argsort()[::-1]\nsorted_features = feature_names[sorted_idx]\nsorted_importances = importances[sorted_idx]\n\n# Create the plot with sorted features and importances\nplt.barh(sorted_features, sorted_importances)\nplt.xlabel('Feature Importance')\nplt.ylabel('Feature')\nplt.title('Random Forest Feature Importance (Sorted)')\nplt.show()\n\ntrain_sizes, train_scores, val_scores = learning_curve(best_rf, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1)\ntrain_scores_mean = train_scores.mean(axis=1)\nval_scores_mean = val_scores.mean(axis=1)\n\nplt.plot(train_sizes, train_scores_mean, label='Training Score')\nplt.plot(train_sizes, val_scores_mean, label='Validation Score')\nplt.xlabel('Training Set Size')\nplt.ylabel('ROC AUC Score')\nplt.title('Learning Curve')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-02T22:21:20.037454Z","iopub.execute_input":"2024-09-02T22:21:20.037902Z","iopub.status.idle":"2024-09-02T22:22:23.512287Z","shell.execute_reply.started":"2024-09-02T22:21:20.037861Z","shell.execute_reply":"2024-09-02T22:22:23.510708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Conclusion: The good prediction scores of linear regression and random forest models on the given columns of data show that workload, tenure, number of projects, and satisfaction levels are significant predictors of voluntary turnover (quitting). The most significant predictor of quitting is satisfaction level, negative correlation between satisfaction level and quitting means that more satisfied employees are less likely to quit.","metadata":{}}]}